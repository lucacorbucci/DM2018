{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pydotplus\n",
    "from sklearn import tree\n",
    "from IPython.display import Image\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os, errno\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import DBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues, save=\"False\", path=\"/home/\"):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    if save is True:\n",
    "        plt.savefig(path+\".pdf\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       false\n",
       "1       false\n",
       "2        true\n",
       "3       false\n",
       "4        true\n",
       "5        true\n",
       "6       false\n",
       "7        true\n",
       "8       false\n",
       "9       false\n",
       "10      false\n",
       "11      false\n",
       "12      false\n",
       "13      false\n",
       "14      false\n",
       "15       true\n",
       "16      false\n",
       "17      false\n",
       "18      false\n",
       "19      false\n",
       "20       true\n",
       "21       true\n",
       "22      false\n",
       "23      false\n",
       "24       true\n",
       "25       true\n",
       "26      false\n",
       "27      false\n",
       "28      false\n",
       "29      false\n",
       "        ...  \n",
       "9968    false\n",
       "9969    false\n",
       "9970    false\n",
       "9971    false\n",
       "9972    false\n",
       "9973    false\n",
       "9974     true\n",
       "9975    false\n",
       "9976    false\n",
       "9977    false\n",
       "9978    false\n",
       "9979    false\n",
       "9980    false\n",
       "9981     true\n",
       "9982    false\n",
       "9983     true\n",
       "9984    false\n",
       "9985    false\n",
       "9986    false\n",
       "9987     true\n",
       "9988    false\n",
       "9989     true\n",
       "9990    false\n",
       "9991    false\n",
       "9992    false\n",
       "9993    false\n",
       "9994    false\n",
       "9995    false\n",
       "9996    false\n",
       "9997    false\n",
       "Name: credit_default, Length: 9998, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced = pd.read_csv(\"./outCompleta.csv\")\n",
    "reduced[\"sums\"] = reduced.iloc[:, 5:10].sum(axis=1)\n",
    "reduced[\"g3\"] = reduced.iloc[:, 5:10].ge(1,axis=1).sum(axis=1)\n",
    "\n",
    "cols = reduced.columns.tolist()\n",
    "cols.remove('age')\n",
    "cols.remove('status')\n",
    "cols.remove('education')\n",
    "cols.remove('sex')\n",
    "cols.remove('credit_default')\n",
    "cols.remove('pa-apr')\n",
    "cols.remove('pa-may')\n",
    "cols.remove('pa-jun')\n",
    "cols.remove('pa-jul')\n",
    "cols.remove('pa-aug')\n",
    "cols.remove('pa-sep')\n",
    "cols.remove('ps-apr')\n",
    "cols.remove('ps-may')\n",
    "cols.remove('ps-jun')\n",
    "cols.remove('ps-jul')\n",
    "cols.remove('ps-aug')\n",
    "cols.remove('ps-sep')\n",
    "cols.remove('sums')\n",
    "cols.remove('ba-apr')\n",
    "cols.remove('ba-may')\n",
    "cols.remove('ba-jun')\n",
    "cols.remove('ba-jul')\n",
    "cols.remove('ba-aug')\n",
    "cols.remove('ba-sep')\n",
    "cols.remove('g3')\n",
    "\n",
    "    \n",
    "reduced = reduced.filter(['g3','ps-sep','ps-apr','ps-jul','ps-aug','ps-jun','ps-may','credit_default'], axis=1)\n",
    "#reduced = reduced.filter(['ps-sep','g3','ps-apr','credit_default'], axis=1)\n",
    "#reduced= reduced.drop(columns=['limit','varps','ps-apr','ps-may','ps-aug','ps-jul','ps-jun','ba-sep','ba-sep','ba-aug','ba-jul','ba-jun','ba-may','ba-apr','pa-sep','pa-aug','pa-jun','pa-may','pa-apr','sums'])\n",
    "#print(reduced.head)\n",
    "reduced['credit_default'].replace({0: 'false', 1: 'true'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spit dataset in Training and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [col for col in reduced.columns if col != 'credit_default']\n",
    "X = reduced[attributes]\n",
    "y = reduced['credit_default']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=100, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3096, 7)\n",
      "(3096,)\n",
      "(3000, 7)\n",
      "(3000,)\n"
     ]
    }
   ],
   "source": [
    "rat=1\n",
    "\n",
    "sm = RandomUnderSampler(ratio=rat, random_state=42)\n",
    "\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "X_train.shape, y_train.shape \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree: Area under the ROC curve = 0.7560373127785114\n",
      "{'criterion': 'entropy', 'max_depth': 6, 'min_samples_leaf': 52, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {'criterion': ['gini', 'entropy'],'min_samples_split' : range(2,300,50),'min_samples_leaf' : range(2,300,50),'max_depth': np.arange(2, 7)}\n",
    "dt1 = GridSearchCV(DecisionTreeClassifier(splitter=\"best\"), param_grid, cv=StratifiedKFold(15), scoring='accuracy')\n",
    "dt1_fit = dt1.fit(X_train, y_train)\n",
    "\n",
    "tree_performance = roc_auc_score(y_test, dt1_fit.predict_proba(X_test)[:, 1])\n",
    "print(\"DecisionTree: Area under the ROC curve = {}\".format(tree_performance))\n",
    "opt_dt1 = dt1_fit.best_estimator_\n",
    "print(\"{}\".format(dt1_fit.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8213 (+/- 0.01)\n",
      "F1-score: 0.6830 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(opt_dt1, X, y, cv=10)\n",
    "print('Accuracy: %0.4f (+/- %0.2f)' % (scores.mean(), scores.std() * 2))\n",
    "cva=('Accuracy: %0.4f (+/- %0.2f)' % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "scores = cross_val_score(opt_dt1, X, y, cv=10, scoring='f1_macro')\n",
    "print('F1-score: %0.4f (+/- %0.2f)' % (scores.mean(), scores.std() * 2))\n",
    "cvf=('F1-score: %0.4f (+/- %0.2f)' % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g3 0.6954539767558132\n",
      "ps-sep 0.16922795145234557\n",
      "ps-apr 0.04990863209284918\n",
      "ps-jul 0.004110678458182943\n",
      "ps-aug 0.06122283938860133\n",
      "ps-jun 0.012823014714496914\n",
      "ps-may 0.007252907137710841\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not numpy.int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4a7f75c4b1c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                 \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_dt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                 \u001b[0mfilled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                 special_characters=False)  \n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/tree/export.py\u001b[0m in \u001b[0;36mexport_graphviz\u001b[0;34m(decision_tree, out_file, max_depth, feature_names, class_names, label, filled, leaves_parallel, impurity, node_ids, proportion, rotate, rounded, special_characters, precision)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;31m# Now recurse the tree and add node & edge attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0mrecurse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# If required, draw leaf nodes at same depth as each other\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/tree/export.py\u001b[0m in \u001b[0;36mrecurse\u001b[0;34m(tree, node_id, criterion, parent, depth)\u001b[0m\n\u001b[1;32m    334\u001b[0m             out_file.write('%d [label=%s'\n\u001b[1;32m    335\u001b[0m                            % (node_id,\n\u001b[0;32m--> 336\u001b[0;31m                               node_to_str(tree, node_id, criterion)))\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/tree/export.py\u001b[0m in \u001b[0;36mnode_to_str\u001b[0;34m(tree, node_id, criterion)\u001b[0m\n\u001b[1;32m    304\u001b[0m                                           \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                                           characters[2])\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mnode_string\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;31m# Clean up any trailing newlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not numpy.int64"
     ]
    }
   ],
   "source": [
    "for col, imp in zip(attributes, opt_dt1.feature_importances_):\n",
    "    print(col, imp)\n",
    "    \n",
    "dot_data = tree.export_graphviz(opt_dt1, out_file=None,  \n",
    "                                feature_names=attributes, \n",
    "                                class_names=opt_dt1.classes_,  \n",
    "                                filled=True, rounded=True,  \n",
    "                                special_characters=False)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = opt_dt1.predict(X_train)\n",
    "a1 = ('Accuracy %s' % accuracy_score(y_train, y_pred1))\n",
    "print(a1)\n",
    "r1 = ('Recall: %s' %  recall_score(y_train, y_pred1, average='weighted'))\n",
    "print(r1)\n",
    "p1 = ('Precision: %s' %  precision_score(y_train, y_pred1, average='weighted'))\n",
    "print(p1)\n",
    "f1=('F1-score %s' % f1_score(y_train, y_pred1, average='weighted'))\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report1 = classification_report(y_train, y_pred1)\n",
    "print(report1)\n",
    "cm1 = confusion_matrix(y_train, y_pred1)\n",
    "plot_confusion_matrix(cm1, classes=opt_dt1.classes_, normalize=True, title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = opt_dt1.predict(X_test)\n",
    "a2=('Accuracy %s' % accuracy_score(y_test, y_pred2))\n",
    "print(a2)\n",
    "r2=('Recall: %s' %  recall_score(y_test, y_pred2, average='weighted'))\n",
    "print(r2)\n",
    "p2=('Precision: %s' %  precision_score(y_test, y_pred2, average='weighted'))\n",
    "print(p2)\n",
    "f2=('F1-score %s' % f1_score(y_test, y_pred2, average='weighted'))\n",
    "print(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report2 = classification_report(y_test, y_pred2)\n",
    "print(report2)\n",
    "cm2 =confusion_matrix(y_test, y_pred2)\n",
    "plot_confusion_matrix(cm2, classes=opt_dt1.classes_, normalize=True, title='Normalized confusion matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
